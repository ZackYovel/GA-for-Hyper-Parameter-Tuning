{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GA.ipynb","provenance":[{"file_id":"https://github.com/ZackYovel/GA-for-Hyper-Parameter-Tuning/blob/MLK/GA.ipynb","timestamp":1601824128861}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ph3loGE-GHr2"},"source":["# Using Genetic Algorithm for Hyper Parameter Tuning\n","\n","## Using data from the kaggle Housing Prices Competition for Kaggle Learn Users\n","\n","Using multiple populations.\n","\n","Competition URL: https://www.kaggle.com/c/home-data-for-ml-course"]},{"cell_type":"code","metadata":{"id":"cRfGNMEoReB5","executionInfo":{"status":"ok","timestamp":1602054181766,"user_tz":-180,"elapsed":3355,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["# Imports\n","import pandas as pd\n","import numpy as np\n","from numpy.random import default_rng\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import Pipeline\n","import matplotlib.pyplot as plt\n","import concurrent.futures\n","import random\n","import json\n","import os\n","import heapq\n","import time\n","import logging\n","import datetime\n"," \n","random_gen = default_rng()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUjHESIPSvHf","executionInfo":{"status":"error","timestamp":1602054183843,"user_tz":-180,"elapsed":5383,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}},"outputId":"b1c09e1a-b8fe-4779-a5d5-e8eab36b690b","colab":{"base_uri":"https://localhost:8080/","height":868}},"source":["# Data load\n","project_files_path = os.path.join(\"drive\", \"My Drive\", \"Colab Notebooks\", \"GA1\")\n"," \n","train = pd.read_csv(os.path.join(project_files_path, \"train.csv\"))\n","X_test = pd.read_csv(os.path.join(project_files_path, \"test.csv\"))\n"," \n","X = train.drop(['SalePrice'], axis=1)\n","y = train.SalePrice\n"," \n","X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.2, random_state=0)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4b89ab230d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mproject_files_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"My Drive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Colab Notebooks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GA1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_files_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_files_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/GA1/train.csv'"]}]},{"cell_type":"code","metadata":{"id":"N6t-9lAcFwlc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiS3QlJ-Vch2","executionInfo":{"status":"aborted","timestamp":1602054183818,"user_tz":-180,"elapsed":5253,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["# Preprocess data\n","\n","numerical_columns = [col for col in X_train.columns if X_train[col].dtype != 'object']\n","categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n","\n","most_frequent_num_imputer = SimpleImputer(strategy='most_frequent')\n","most_frequent_cat_imputer = SimpleImputer(strategy='most_frequent')\n","\n","low_cardinality_cols = [col for col in categorical_columns if X_train[col].nunique() < 10]\n","high_cardinality_cols = set(categorical_columns) - set(low_cardinality_cols)\n","good_label_cols = [col for col in high_cardinality_cols if set(X_train[col]).issuperset(set(X_cv[col]))]\n","\n","OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","\n","ordinal_encoder = OrdinalEncoder()\n","\n","numerical_transformer = most_frequent_num_imputer\n","\n","categorical_low_card_transformer = Pipeline(\n","    steps=[\n","           ('impute', most_frequent_cat_imputer),\n","           ('encode', OH_encoder)\n","    ]\n",")\n","\n","categorical_high_card_transformer = Pipeline(\n","    steps=[\n","           ('impute', most_frequent_cat_imputer),\n","           ('encode', ordinal_encoder)\n","    ]\n",")\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","                  ('num', numerical_transformer, numerical_columns),\n","                  ('cat_low_card', categorical_low_card_transformer, low_cardinality_cols),\n","                  ('cat_high_card', categorical_high_card_transformer, good_label_cols)\n","    ]\n",")\n","\n","\n","X_train_prepped = preprocessor.fit_transform(X_train)\n","X_cv_prepped = preprocessor.transform(X_cv)\n","X_test_prepped = preprocessor.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG6Jva_-ksV6","executionInfo":{"status":"aborted","timestamp":1602054183829,"user_tz":-180,"elapsed":5244,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["def model_from_hp(hp):\n","  return RandomForestRegressor(n_estimators=hp['n_estimators'],\n","                               criterion='mae',\n","                               max_depth=hp['max_depth'],\n","                               min_samples_split=hp['min_samples_split'],\n","                               min_samples_leaf=hp['min_samples_leaf'],\n","                               min_weight_fraction_leaf=hp['min_weight_fraction_leaf'],\n","                               max_features=hp['max_features'],\n","                               max_leaf_nodes=hp['max_leaf_nodes'],\n","                               min_impurity_decrease=hp['min_impurity_decrease'],\n","                               bootstrap=hp['bootstrap'],\n","                               oob_score=hp['oob_score'],\n","                               n_jobs=-1,\n","                               ccp_alpha=hp['ccp_alpha'],\n","                               max_samples=hp['max_samples'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LurZjShuvzal"},"source":["## Base Notebook: Alpha Male with Harem \n","\n","Alpha male: multiple populations (`n_territories=5, pop_size=30, n_survivors=15`),\n","the best individual parenting every ofspring.\n","\n","Harem: the best individual is paired with every other survivor in a loop until population is full (requires the number of offsprings which is\n","`pop_size - n_survivors` to be >= n_survivors - 1 in order to guarantee all survivors parent offsprings.\n","\n","Small step mutation: 0.01\n","Large step mutation: 0.1\n","\n","v1.0:\n","* inserting evolving mutation rates as advised in:\n","Evolutionary computation: An overview\n","Mitchell, Melanie; Taylor, Charles E. Annual Review of Ecology and Systematics; Palo Alto Vol. 30,  (1999): 593.\n","\n","* Also removing drifts because previous data indicates lack of correlation between being the territory with the best fitness (MAE) value and having the best chance to find a new best fitness value.\n","\n","v1.3\n","* Change the n_estimators gene to have hard limits - to never receive values outside the limits. This is to avoid extremely long runtimes such as over two hours for a generation.\n","\n","v1.5\n","* Change all genes to have hard limits.\n","\n","v1.7\n","* Create fitness function that encourages short runtimes (in minutes)\n","\n","v1.7.1\n","* Test runtime in seconds instead of minutes"]},{"cell_type":"code","metadata":{"id":"Leogj9ScdHRB","executionInfo":{"status":"aborted","timestamp":1602054183833,"user_tz":-180,"elapsed":5188,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["# Hyper parameter tuning (using a genetic algorithm)\n","\n","n_features = X_train_prepped.shape[1]\n","n_samples = X_train_prepped.shape[0]\n","pop_size = 30\n","n_generations = 500\n","n_territories = 5\n","n_survivors = 15\n","drift_threshold = 0.0\n","\n","param_limits = {\n","    'n_estimators':             (1, 1000),\n","    'max_depth':                (1, n_features),\n","    'min_samples_split':        (2, 1000),\n","    'min_samples_leaf':         (1, 1000),\n","    'min_weight_fraction_leaf': (0, 0.5),\n","    'max_features':             (1, n_features),\n","    'max_leaf_nodes':           (2, 10000),\n","    'min_impurity_decrease':    (0, 1),\n","    'bootstrap':                (True, False),\n","    'oob_score':                (True, False),\n","    'ccp_alpha':                (0, 1),\n","    'max_samples':              (1, n_samples),\n","    'small_step_mutation':      (0, 1),\n","    'large_step_mutation':      (0, 1),\n","}\n","\n","\"\"\"\n","Format of ecosystem is:\n","{\n","  'last_generation': int,\n","  'times': [float,...],\n","  'improvements': [float,...],\n","  'average_total_improve': [float,...],\n","  'runtime_running_avg': float,\n","  'total_runtime': float,\n","  'need_drift': [False,...],\n","  'drifted_last_generation': [False,...],\n","  'best_mae_changes': [(float, int),...],\n","  'best_mae': [float,...],\n","  'best_fitness': [float,...],\n","  'std': [float,...],\n","  'avg_mae_survivors': [{\n","    'generation': int,\n","    'values': [None] * n_territories,\n","  }],\n","  'territories': [\n","    [{hp:{}, mae=None, fitness=None},...],\n","    .\n","    .\n","    .\n","  ]\n","}\n","\"\"\"\n","ecosystem = {\n","    'last_generation': 0,\n","    'times': [],\n","    'improvements': [],\n","    'average_total_improve': [],\n","    'runtime_running_avg': 0,\n","    'total_runtime': 0,\n","    'need_drift': [False] * n_territories,\n","    'drifted_last_generation': [False] * n_territories,\n","    'best_mae_changes': [],\n","    'best_mae': [],\n","    'best_fitness': [],\n","    'std': [],\n","    'avg_mae_survivors': [],\n","    'territories': []\n","}\n","\n","\n","def init_env(ecosystem):\n","  territories = []\n","  for i in range(n_territories):\n","    territories.append([])\n","  ecosystem['territories'] = territories\n","\n","\n","def init_HP():\n","  \"\"\"\n","  Provides a randomly initialized set of hyper parameters for a RandomForestRegressor\n","  \"\"\"\n","\n","  global n_features, n_samples, param_limits\n","\n","  HP = {\n","      'n_estimators': int(random.randint(*param_limits['n_estimators'])),\n","      'max_depth': int(random.randint(*param_limits['max_depth'])),\n","      'min_samples_split': int(random.randint(*param_limits['min_samples_split'])),\n","      'min_samples_leaf': int(random.randint(*param_limits['min_samples_leaf'])),\n","      'min_weight_fraction_leaf': random.uniform(*param_limits['min_weight_fraction_leaf']),\n","      'max_features': int(random.randint(*param_limits['max_features'])),\n","      'max_leaf_nodes': int(random.randint(*param_limits['max_leaf_nodes'])),\n","      'min_impurity_decrease': random.uniform(*param_limits['min_impurity_decrease']),\n","      'bootstrap': bool(random.choice((True, False))),\n","      'oob_score': False,\n","      'ccp_alpha': random.uniform(*param_limits['ccp_alpha']),\n","      'max_samples': int(random.randint(*param_limits['max_samples'])),\n","      'small_step_mutation': random.uniform(*param_limits['small_step_mutation']),\n","      'large_step_mutation': random.uniform(*param_limits['large_step_mutation']),\n","  }\n","\n","  if HP['bootstrap'] == True:\n","    HP['oob_score'] = bool(random.choice((True, False)))\n","  \n","  return HP\n","\n","\n","def evaluate_model(model, X_train, X_cv, y_train, y_cv):\n","  model.fit(X_train_prepped, y_train)\n","  pred = model.predict(X_cv_prepped)\n","  return mean_absolute_error(pred, y_cv)\n","\n","\n","def fitness(model, X_train, X_cv, y_train, y_cv):\n","  start_time = time.time()\n","  mae = evaluate_model(model, X_train, X_cv, y_train, y_cv)\n","  duration = time.time() - start_time\n","  fitness = mae + duration\n","  return mae, fitness\n","\n","\n","def compute_sd(limits, zygote):\n","  return (limits[1] - limits[0]) * zygote['small_step_mutation']\n","\n","\n","def small_step_mutation(zygote):\n","  zygote['n_estimators'] = min(max(param_limits['n_estimators'][0], zygote['n_estimators'] + int(np.random.normal(0, compute_sd(param_limits['n_estimators'], zygote)))), param_limits['n_estimators'][1])\n","  zygote['max_depth'] = min(max(param_limits['max_depth'][0], zygote['max_depth'] + int(np.random.normal(0, int(compute_sd(param_limits['max_depth'], zygote))))), param_limits['max_depth'][1])\n","  zygote['min_samples_split'] = min(max(param_limits['min_samples_split'][0], zygote['min_samples_split'] + int(np.random.normal(0, compute_sd(param_limits['min_samples_split'], zygote)))), param_limits['min_samples_split'][1])\n","  zygote['min_samples_leaf'] = min(max(param_limits['min_samples_leaf'][0], zygote['min_samples_leaf'] + int(np.random.normal(0, compute_sd(param_limits['min_samples_leaf'], zygote)))), param_limits['min_samples_leaf'][1])\n","  zygote['min_weight_fraction_leaf'] = min(max(param_limits['min_weight_fraction_leaf'][0], zygote['min_weight_fraction_leaf'] + np.random.normal(0, compute_sd(param_limits['min_weight_fraction_leaf'], zygote))), param_limits['min_weight_fraction_leaf'][1])\n","  zygote['max_features'] = min(max(param_limits['max_features'][0], zygote['max_features'] + int(np.random.normal(0, int(compute_sd(param_limits['max_features'], zygote))))), param_limits['max_features'][1])\n","  zygote['max_leaf_nodes'] = min(max(param_limits['max_leaf_nodes'][0], zygote['max_leaf_nodes'] + int(np.random.normal(0, compute_sd(param_limits['max_leaf_nodes'], zygote)))), param_limits['max_leaf_nodes'][1])\n","  zygote['min_impurity_decrease'] += min(max(param_limits['min_impurity_decrease'][0], np.random.normal(0, compute_sd(param_limits['min_impurity_decrease'], zygote))), param_limits['min_impurity_decrease'][1])\n","  zygote['bootstrap'] = bool(np.random.choice([True, False], p=[1 - zygote['small_step_mutation'] if x==zygote['bootstrap'] else zygote['small_step_mutation'] for x in [True, False]]))\n","  zygote['oob_score'] = bool(np.random.choice([True, False], p=[1 - zygote['small_step_mutation'] if x==zygote['oob_score'] else zygote['small_step_mutation'] for x in [True, False]]))\n","  zygote['ccp_alpha'] = min(max(param_limits['ccp_alpha'][0], zygote['ccp_alpha'] + np.random.normal(0, compute_sd(param_limits['ccp_alpha'], zygote))), param_limits['ccp_alpha'][1])\n","  zygote['max_samples'] = min(max(param_limits['max_samples'][0], zygote['max_samples'] + int(np.random.normal(0, compute_sd(param_limits['max_samples'], zygote)))), param_limits['max_samples'][1])\n","  zygote['large_step_mutation'] = min(max(param_limits['large_step_mutation'][0], zygote['large_step_mutation'] + np.random.normal(0, compute_sd(param_limits['large_step_mutation'], zygote))), param_limits['large_step_mutation'][1])\n","  zygote['small_step_mutation'] = min(max(param_limits['small_step_mutation'][0], zygote['small_step_mutation'] + np.random.normal(0, compute_sd(param_limits['small_step_mutation'], zygote))), param_limits['small_step_mutation'][1])\n","\n","  if zygote['bootstrap'] == False:\n","    zygote['oob_score'] = False\n","  \n","  return zygote\n","\n","\n","def should_do_large_step(zygote):\n","  return np.random.choice([True, False], p=[1 - zygote['large_step_mutation'], zygote['large_step_mutation']])\n","\n","\n","def large_step_mutation(zygote):\n","  global param_limits\n","\n","  if should_do_large_step(zygote):\n","    zygote['n_estimators'] = int(random.randint(*param_limits['n_estimators']))\n","  if should_do_large_step(zygote):\n","    zygote['max_depth'] = int(random.randint(*param_limits['max_depth']))\n","  if should_do_large_step(zygote):\n","    zygote['min_samples_split'] = int(random.randint(*param_limits['min_samples_split']))\n","  if should_do_large_step(zygote):\n","    zygote['min_samples_leaf'] = int(random.randint(*param_limits['min_samples_leaf']))\n","  if should_do_large_step(zygote):\n","    zygote['min_weight_fraction_leaf'] = random.uniform(*param_limits['min_weight_fraction_leaf'])\n","  if should_do_large_step(zygote):\n","    zygote['max_features'] = int(random.randint(*param_limits['max_features']))\n","  if should_do_large_step(zygote):\n","    zygote['max_leaf_nodes'] = int(random.randint(*param_limits['max_leaf_nodes']))\n","  if should_do_large_step(zygote):\n","    zygote['min_impurity_decrease'] = random.uniform(*param_limits['min_impurity_decrease'])\n","  if should_do_large_step(zygote):\n","    zygote['bootstrap'] = bool(random.choice((True, False)))\n","  if should_do_large_step(zygote):\n","    zygote['oob_score'] = bool(random.choice((True, False)))\n","  if should_do_large_step(zygote):\n","    zygote['ccp_alpha'] = random.uniform(*param_limits['ccp_alpha'])\n","  if should_do_large_step(zygote):\n","    zygote['max_samples'] = int(random.randint(*param_limits['max_samples']))\n","  if should_do_large_step(zygote):\n","    zygote['small_step_mutation'] = random.uniform(*param_limits['small_step_mutation'])\n","  if should_do_large_step(zygote):\n","    zygote['large_step_mutation'] = random.uniform(*param_limits['large_step_mutation'])\n","\n","  if zygote['bootstrap'] == False:\n","    zygote['oob_score'] = False\n","  \n","  return zygote\n","\n","\n","def mutate(zygote):\n","  zygote = small_step_mutation(zygote)\n","  zygote = large_step_mutation(zygote)\n","  return zygote\n","\n","\n","def fertilize(male, female):\n","  parents = [male, female]\n","\n","  return mutate({\n","      'n_estimators': random.choice(parents)['hp']['n_estimators'],\n","      'max_depth': random.choice(parents)['hp']['max_depth'],\n","      'min_samples_split': random.choice(parents)['hp']['min_samples_split'],\n","      'min_samples_leaf': random.choice(parents)['hp']['min_samples_leaf'],\n","      'min_weight_fraction_leaf': random.choice(parents)['hp']['min_weight_fraction_leaf'],\n","      'max_features': random.choice(parents)['hp']['max_features'],\n","      'max_leaf_nodes': random.choice(parents)['hp']['max_leaf_nodes'],\n","      'min_impurity_decrease': random.choice(parents)['hp']['min_impurity_decrease'],\n","      'bootstrap': random.choice(parents)['hp']['bootstrap'],\n","      'oob_score': random.choice(parents)['hp']['oob_score'],\n","      'ccp_alpha': random.choice(parents)['hp']['ccp_alpha'],\n","      'max_samples': random.choice(parents)['hp']['max_samples'],\n","      'small_step_mutation': random.choice(parents)['hp']['small_step_mutation'],\n","      'large_step_mutation': random.choice(parents)['hp']['large_step_mutation'],\n","  })\n","\n","\n","def breed(population):\n","  while len(population) < pop_size:\n","    for survivor in population[1:n_survivors]:\n","      population.append({'hp': fertilize(population[0], survivor), 'mae': None, 'fitness': None})\n","      if len(population) >= pop_size:\n","        break\n","\n","\n","def perform_drift(territories, idx):\n","  territory = territories[idx]\n","  other_territories = [(i, x) for (i, x) in enumerate(territories) if x is not territory]\n","  random.shuffle(other_territories)\n","  for i, other_ter in other_territories:\n","    if other_ter[1]['fitness'] < territory[0]['fitness']:\n","      other_ind = other_ter[1]\n","      territory[0] = other_ind\n","      other_ter.pop(1)\n","      print(\"Drift from ter.\", i, \"to ter.\", idx)\n","      break\n","\n","\n","class Survivor:\n","  def __init__(self, hp, mae, fitness):\n","    self.hp = hp\n","    self.mae = mae\n","    self.fitness = fitness\n","  \n","  def __lt__(self, other):\n","    return self.fitness < other.fitness\n","  \n","  def to_dict(self):\n","    return {'hp': self.hp, 'mae': self.mae, 'fitness': self.fitness}\n","\n","\n","def seconds2timestr(seconds): \n","  hour = seconds // 3600\n","  seconds %= 3600\n","  minutes = seconds // 60\n","  seconds %= 60\n","  \n","  return \"%d:%02d:%02d\" % (hour, minutes, seconds)\n","\n","\n","def process_specimen(population, idx):\n","  global preped_X_train, preped_X_cv, y_train, y_cv\n","  if population[idx]['mae'] is None:\n","    model = model_from_hp(population[idx]['hp'])\n","    population[idx]['mae'], population[idx]['fitness'] = fitness(model, X_train_prepped, X_cv_prepped, y_train, y_cv)\n","  return population[idx]\n","\n","\n","def process_teritory(ecosystem, idx, executor):\n","    territories = ecosystem['territories']\n","    futures = [executor.submit(process_specimen, territories[idx], i) for i in range(len(territories[idx]))]\n","    survivors = []\n","    for future in concurrent.futures.as_completed(futures):\n","        heapq.heappush(survivors, Survivor(**future.result()))\n","        print('.', end='')\n","    survivors = heapq.nsmallest(n_survivors, survivors)\n","    new_avg_mae_survivors = np.mean([x.mae for x in survivors])\n","    try:\n","      improvement = ecosystem['avg_mae_survivors'][-2]['values'][idx] / new_avg_mae_survivors - 1\n","      if n_territories > 1 and improvement <= drift_threshold:\n","        ecosystem['need_drift'][idx] = True\n","    except Exception as ex:\n","      improvement = 0\n","    ecosystem['avg_mae_survivors'][-1]['values'][idx] = new_avg_mae_survivors\n","    print(\"\\nBest fitness for territory\", idx, \"is\", survivors[0].mae, \". Average improvement is\", improvement)\n","    territories[idx] = [x.to_dict() for x in survivors]\n","    return improvement\n","\n","\n","try:\n","  with open(os.path.join(project_files_path, 'ecosystem.json')) as f:\n","    ecosystem = json.load(f)\n","    generations_passed = ecosystem['last_generation'] + 1\n","except Exception as e:\n","  for i in range(n_territories):\n","    ecosystem['territories'].append([{'hp': init_HP(), 'mae': None, 'fitness': None} for i in range(int(pop_size))])\n","  generations_passed = 0\n","\n","# Supress warnings to avoid terminal clutter\n","logging.captureWarnings(True)\n","\n","\n","with concurrent.futures.ThreadPoolExecutor() as executor:\n","  for generation in range(generations_passed, n_generations):\n","    print(\"*\" * 100, \"\\nGeneration\", str(generation) + '/' + str(n_generations), datetime.datetime.now(), \"time is 3 hours late\")\n","    start_time = time.time()\n","    ecosystem['avg_mae_survivors'].append({'generation': generation, 'values':[0] * n_territories})\n","    improvements = []\n","    futures = [executor.submit(process_teritory, ecosystem, idx, executor) for idx in range(len(ecosystem['territories']))]\n","    for future in concurrent.futures.as_completed(futures):\n","      improvements.append(future.result())\n","    average_improve = float(np.mean(improvements))\n","    try:\n","      ecosystem['average_total_improve'].append(ecosystem['average_total_improve'][-1] + average_improve)\n","    except:\n","      ecosystem['average_total_improve'].append(average_improve)\n","    ecosystem['improvements'].append(average_improve)\n","\n","    # Get best mae and fitness\n","    best_mae = ecosystem['territories'][0][0]['mae']\n","    best_fitness = ecosystem['territories'][0][0]['fitness']\n","    ter = 0\n","    for i, territory in enumerate(ecosystem['territories']):\n","      if territory[0]['mae'] < best_mae:\n","        best_mae = territory[0]['mae']\n","        ter = i\n","      if territory[0]['fitness'] < best_fitness:\n","        best_fitness = territory[0]['fitness']\n","    \n","    ecosystem['best_mae'].append(best_mae)\n","    ecosystem['best_fitness'].append(best_fitness)\n","    if len(ecosystem['best_mae_changes']) == 0 or best_mae < ecosystem['best_mae_changes'][-1][0]:\n","      ecosystem['best_mae_changes'].append((best_mae, ter))\n","    # End get best mae and fitness\n","\n","    # # Drifts\n","    # if n_territories > 1:\n","    #   for i in range(len(ecosystem['need_drift'])):\n","    #     if ecosystem['need_drift'][i] and not ecosystem['drifted_last_generation'][i]:\n","    #       perform_drift(ecosystem['territories'], i)\n","    #       ecosystem['drifted_last_generation'][i] = True\n","    #     elif ecosystem['drifted_last_generation'][i] == True:\n","    #       ecosystem['drifted_last_generation'][i] = False\n","    # # End drifts\n","\n","    # Bread\n","    futures = [executor.submit(breed, ecosystem['territories'][i]) for i in range(n_territories)]\n","    for future in concurrent.futures.as_completed(futures):\n","      future.result() # just make sure all breedings are done before continuing\n","    # End breeding\n","\n","    # Compute standard deviation of all individuals\n","    fitness_vals = []\n","    for territory in ecosystem['territories']:\n","      for individual in territory:\n","        if individual['fitness'] is not None:\n","          fitness_vals.append(individual['fitness'])\n","    ecosystem['std'].append(np.std(fitness_vals))\n","    # End compute standard deviation\n","\n","    ecosystem['last_generation'] = generation\n","    end_time = time.time()\n","    generation_process_time = end_time - start_time\n","    ecosystem['total_runtime'] += generation_process_time\n","    ecosystem['runtime_running_avg'] = (ecosystem['runtime_running_avg'] + generation_process_time) / 2\n","    ecosystem['times'].append(generation_process_time)\n","    with open(os.path.join(project_files_path, 'ecosystem.json'), 'w') as f:\n","      json.dump(ecosystem, f)\n","    remaining_generations = n_generations - generation - 1\n","    print(\"This generation took\", seconds2timestr(generation_process_time), \"to process.\")\n","    print(\"Remaining\", remaining_generations, \"generations and approximately\", seconds2timestr(ecosystem['runtime_running_avg'] * remaining_generations))\n","    print(\"Average improvement for generation:\", average_improve, \". Average total improvement:\", ecosystem['average_total_improve'][-1])\n","    print(\"Total runtime:\", seconds2timestr(ecosystem['total_runtime']))\n","    \n","    plt.figure(figsize=(8, 6))\n","\n","    plt.subplot(2, 2, 1)\n","    plt.plot(ecosystem['improvements'])\n","    plt.xlabel(\"Generation\")\n","    plt.ylabel(\"Avg. improvement per gen.\")\n","\n","    plt.subplot(2, 2, 2)\n","    plt.plot(ecosystem['average_total_improve'])\n","    plt.xlabel(\"Generation\")\n","    plt.ylabel(\"Total improvement\")\n","\n","    plt.subplot(2, 2, 3)\n","    plt.plot(ecosystem['best_mae'])\n","    plt.xlabel(\"Generation\")\n","    plt.ylabel(\"Best mae\")\n","\n","    plt.subplot(2, 2, 4)\n","    plt.plot(ecosystem['std'])\n","    plt.xlabel(\"Generation\")\n","    plt.ylabel(\"Standard deviation\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    plt.plot(ecosystem['best_fitness'])\n","    plt.xlabel(\"Generation\")\n","    plt.ylabel(\"Best fitness\")\n","    plt.show()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNpu1HU4zQpm","executionInfo":{"status":"aborted","timestamp":1602054183837,"user_tz":-180,"elapsed":5174,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["# Prepeare test predictions\n","with open(os.path.join(project_files_path, 'ecosystem.json')) as f:\n","    ecosystem = json.load(f)\n","\n","X_train_cv = np.concatenate([X_train_prepped, X_cv_prepped], axis=0)\n","y_train_cv = np.concatenate([y_train, y_cv], axis=0)\n","\n","best_hp = ecosystem['territories'][0][0]['hp']\n","best_mae = ecosystem['territories'][0][0]['mae']\n","for teritory in ecosystem['territories']:\n","  for specimen in teritory:\n","    if specimen['mae'] is not None and specimen['mae'] < best_mae:\n","      best_mae = specimen['mae']\n","      best_hp = specimen['hp']\n","\n","print(best_hp)\n","\n","model = model_from_hp(best_hp)\n","\n","print(\"Fitting\")\n","\n","model.fit(X_train_cv, y_train_cv)\n","\n","print(\"Predicting\")\n","\n","preds_test = model.predict(X_test_prepped)\n","\n","print(\"Saving\")\n","\n","output = pd.DataFrame({'Id': X_test.Id.astype('int32'),\n","                       'SalePrice': preds_test})\n","output.to_csv(os.path.join(project_files_path, 'submission_gen.csv'), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGzeA83Xo6UJ","executionInfo":{"status":"aborted","timestamp":1602054183840,"user_tz":-180,"elapsed":5157,"user":{"displayName":"Yekhezkel Yovel","photoUrl":"","userId":"06753137433584351349"}}},"source":["output = pd.DataFrame({'Id': X_test.Id.astype('int32'),\n","                       'SalePrice': preds_test})\n","output.to_csv(os.path.join(project_files_path, 'submission_gen.csv'), index=False)"],"execution_count":null,"outputs":[]}]}