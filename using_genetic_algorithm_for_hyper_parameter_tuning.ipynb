{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "using_genetic_algorithm_for_hyper_parameter_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1AE5xlsc0eQ0mGJneExkvZOQMEojVRPdi",
      "authorship_tag": "ABX9TyN6YEWW4p7v7wlE/Wkk4SGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZackYovel/using_genetic_algorithm_for_hyper_parameter_tuning/blob/MLK/using_genetic_algorithm_for_hyper_parameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph3loGE-GHr2"
      },
      "source": [
        "# Using Genetic Algorithm for Hyper Parameter Tuning\n",
        "\n",
        "## Using data from the kaggle Housing Prices Competition for Kaggle Learn Users\n",
        "\n",
        "Using multiple populations.\n",
        "\n",
        "Competition URL: https://www.kaggle.com/c/home-data-for-ml-course"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRfGNMEoReB5"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import concurrent.futures\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import heapq\n",
        "import time\n",
        "import logging\n",
        "\n",
        "random_gen = default_rng()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUjHESIPSvHf"
      },
      "source": [
        "# Data load\n",
        "project_files_path = os.path.join(\"drive\", \"My Drive\", \"Colab Notebooks\", \"Using Genetic Algorithm for Hyper Parameter Tuning\")\n",
        "\n",
        "train = pd.read_csv(os.path.join(project_files_path, \"train.csv\"))\n",
        "X_test = pd.read_csv(os.path.join(project_files_path, \"test.csv\"))\n",
        "\n",
        "X = train.drop(['SalePrice'], axis=1)\n",
        "y = train.SalePrice\n",
        "\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiS3QlJ-Vch2",
        "outputId": "fe932f67-cd23-4d95-9e32-4dcaf110d848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Preprocess data\n",
        "\n",
        "num_X_train = X_train.select_dtypes(exclude=['object'])\n",
        "num_X_cv = X_cv.select_dtypes(exclude=['object'])\n",
        "num_X_test = X_test.select_dtypes(exclude=['object'])\n",
        "\n",
        "cat_X_train = X_train.select_dtypes(include=['object'])\n",
        "cat_X_cv = X_cv.select_dtypes(include=['object'])\n",
        "cat_X_test = X_test.select_dtypes(include=['object'])\n",
        "\n",
        "# Imputation\n",
        "most_frequent_num_imputer = SimpleImputer(strategy='most_frequent')\n",
        "imputed_num_X_train = pd.DataFrame(most_frequent_num_imputer.fit_transform(num_X_train))\n",
        "imputed_num_X_cv = pd.DataFrame(most_frequent_num_imputer.transform(num_X_cv))\n",
        "imputed_num_X_test = pd.DataFrame(most_frequent_num_imputer.transform(num_X_test))\n",
        "\n",
        "imputed_num_X_train.columns = num_X_train.columns\n",
        "imputed_num_X_cv.columns = num_X_cv.columns\n",
        "imputed_num_X_test.columns = num_X_test.columns\n",
        "\n",
        "imputed_num_X_train.index = num_X_train.index\n",
        "imputed_num_X_cv.index = num_X_cv.index\n",
        "imputed_num_X_test.index = num_X_test.index\n",
        "\n",
        "most_frequent_cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "imputed_cat_X_train = pd.DataFrame(most_frequent_cat_imputer.fit_transform(cat_X_train))\n",
        "imputed_cat_X_cv = pd.DataFrame(most_frequent_cat_imputer.transform(cat_X_cv))\n",
        "imputed_cat_X_test = pd.DataFrame(most_frequent_cat_imputer.transform(cat_X_test))\n",
        "\n",
        "imputed_cat_X_train.columns = cat_X_train.columns\n",
        "imputed_cat_X_cv.columns = cat_X_cv.columns\n",
        "imputed_cat_X_test.columns = cat_X_test.columns\n",
        "\n",
        "imputed_cat_X_train.index = cat_X_train.index\n",
        "imputed_cat_X_cv.index = cat_X_cv.index\n",
        "imputed_cat_X_test.index = cat_X_test.index\n",
        "\n",
        "# Categorical data handeling\n",
        "\n",
        "# OH encoding of low cardinality categorical features (<= 10 unique values)\n",
        "categorical_columns = [col for col in X_train.columns if X_train[col].dtype=='object']\n",
        "low_cardinality_cols = [col for col in categorical_columns if X_train[col].nunique() < 10]\n",
        "low_card_X_train = imputed_cat_X_train[low_cardinality_cols]\n",
        "low_card_X_cv = imputed_cat_X_cv[low_cardinality_cols]\n",
        "low_card_X_test = imputed_cat_X_test[low_cardinality_cols]\n",
        "\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_encoded_X_train = pd.DataFrame(OH_encoder.fit_transform(low_card_X_train))\n",
        "OH_encoded_X_cv = pd.DataFrame(OH_encoder.transform(low_card_X_cv))\n",
        "OH_encoded_X_test = pd.DataFrame(OH_encoder.transform(low_card_X_test))\n",
        "\n",
        "OH_encoded_X_train.index = low_card_X_train.index\n",
        "OH_encoded_X_cv.index = low_card_X_cv.index\n",
        "OH_encoded_X_test.index = low_card_X_test.index\n",
        "\n",
        "# Label encoding of high cardinality categorical features when possible\n",
        "high_cardinality_cols = set(categorical_columns) - set(low_cardinality_cols)\n",
        "\n",
        "good_label_cols = [col for col in high_cardinality_cols if set(imputed_cat_X_train[col]).issuperset(set(imputed_cat_X_cv[col]))]\n",
        "bad_label_cols = list(set(high_cardinality_cols) - set(good_label_cols))\n",
        "\n",
        "label_X_train = imputed_cat_X_train[good_label_cols]\n",
        "label_X_cv = imputed_cat_X_cv[good_label_cols]\n",
        "label_X_test = imputed_cat_X_test[good_label_cols]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "for col in good_label_cols:\n",
        "  label_X_train[col] = label_encoder.fit_transform(label_X_train[col])\n",
        "  label_X_cv[col] = label_encoder.transform(label_X_cv[col])\n",
        "  label_X_test[col] = label_encoder.transform(label_X_test[col])\n",
        "\n",
        "preped_X_train = pd.concat([imputed_num_X_train, OH_encoded_X_train, label_X_train], axis=1)\n",
        "preped_X_cv = pd.concat([imputed_num_X_cv, OH_encoded_X_cv, label_X_cv], axis=1)\n",
        "preped_X_test = pd.concat([imputed_num_X_test, OH_encoded_X_test, label_X_test], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG6Jva_-ksV6"
      },
      "source": [
        "def model_from_hp(hp):\n",
        "  return RandomForestRegressor(n_estimators=hp['n_estimators'],\n",
        "                               criterion='mae',\n",
        "                               max_depth=hp['max_depth'],\n",
        "                               min_samples_split=hp['min_samples_split'],\n",
        "                               min_samples_leaf=hp['min_samples_leaf'],\n",
        "                               min_weight_fraction_leaf=hp['min_weight_fraction_leaf'],\n",
        "                               max_features=hp['max_features'],\n",
        "                               max_leaf_nodes=hp['max_leaf_nodes'],\n",
        "                               min_impurity_decrease=hp['min_impurity_decrease'],\n",
        "                               bootstrap=hp['bootstrap'],\n",
        "                               oob_score=hp['oob_score'],\n",
        "                               n_jobs=-1,\n",
        "                               ccp_alpha=hp['ccp_alpha'],\n",
        "                               max_samples=hp['max_samples'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNEYpEtNHbw1"
      },
      "source": [
        "## Hyper Parameter Tuning Using a Genetic Algorithm\n",
        "\n",
        "### Algorithm Description:\n",
        "1. Prepare generation 0: initialize n_teritories teritories each of which has pop_size instances of hyper parameter sets with random values in it's population.\n",
        "2. For each generation (steps 3-16):\n",
        "3. For each teritory (steps 4-14):\n",
        "4. For each specimen in the population (steps 5-7):\n",
        "5. Build a model based on the specimen\n",
        "6. Evaluate the model\n",
        "7. While unevaluated specimen remain go back to step 4.\n",
        "8. Breed population until it reaches pop_size:\n",
        "9. While population size is less than pop_size (steps 10-13):\n",
        "10. Randomly select two distinct parent specimen (give higher chances to reproduce to specimen with better fit value).\n",
        "11. Create a new specimen by randomly \"inheriting\" hyper parameter values from the parents\n",
        "12. Mutate the new specimen:\n",
        "for numerical hyper parameters add a value chosen randomly from a normal ditribution with mean 0 and standard deviation of approximately mutation_factor times the size of the range of legal values.\n",
        "for boolean hyper parameters choose randomly giving the existing value a probability of 1 - mutation_factor to be chosen again.\n",
        "13. While population size is less than pop_size return to step 9\n",
        "14. While not all teritories processed return to step 3\n",
        "15. Compute: mutation_factor = 0.1 * (number of teritories that need drift + 1)\n",
        "(a population needs drift if the average improvement of it's n_survivors best specimen is less than drift_threshold)\n",
        "16. While not processed all generations go back to step 2.\n",
        "17. Return best set of hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leogj9ScdHRB"
      },
      "source": [
        "# Hyper parameter tuning (using a genetic algorithm)\n",
        "\n",
        "n_features = len(preped_X_train.columns)\n",
        "n_samples = preped_X_train.shape[0]\n",
        "pop_size = 25\n",
        "n_generations = 500\n",
        "n_teritories = 6\n",
        "# MIN_SURVIVORS = 5\n",
        "n_survivors = 20\n",
        "drift_threshold = 0.001\n",
        "small_step_mutation_rate = 0.1\n",
        "large_step_mutation_rate = 0.01\n",
        "\n",
        "param_limits = {\n",
        "    'n_estimators':             (1, 1000),\n",
        "    'max_depth':                (1, n_features),\n",
        "    'min_samples_split':        (2, 1000),\n",
        "    'min_samples_leaf':         (1, 1000),\n",
        "    'min_weight_fraction_leaf': (0, 0.5),\n",
        "    'max_features':             (1, n_features),\n",
        "    'max_leaf_nodes':           (2, 10000),\n",
        "    'min_impurity_decrease':    (0, 1),\n",
        "    'bootstrap':                (True, False),\n",
        "    'oob_score':                (True, False),\n",
        "    'ccp_alpha':                (0, 1),\n",
        "    'max_samples':              (1, n_samples),\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "Format of ecosystem is:\n",
        "{\n",
        "  'last_generation': int,\n",
        "  'times': [float,...],\n",
        "  'improvements': [float,...],\n",
        "  'average_total_improve': [float,...],\n",
        "  'runtime_running_avg': float,\n",
        "  'avg_mae_survivors': [{\n",
        "    'generation': int,\n",
        "    'values': [None] * n_teritories,\n",
        "  }],\n",
        "  'teritories': [\n",
        "    [{hp:{}, mae=None},...],\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "ecosystem = {\n",
        "    'last_generation': 0,\n",
        "    'times': [],\n",
        "    'improvements': [],\n",
        "    'average_total_improve': [0],\n",
        "    'runtime_running_avg': 0,\n",
        "    'avg_mae_survivors': [],\n",
        "    'teritories': []\n",
        "}\n",
        "\n",
        "\n",
        "def init_env(ecosystem):\n",
        "  teritories = []\n",
        "  for i in range(n_teritories):\n",
        "    teritories.append([])\n",
        "  ecosystem['teritories'] = teritories\n",
        "\n",
        "\n",
        "def init_HP():\n",
        "  \"\"\"\n",
        "  Provides a randomly initialized set of hyper parameters for a RandomForestRegressor\n",
        "  \"\"\"\n",
        "\n",
        "  global n_features, n_samples, param_limits\n",
        "\n",
        "  HP = {\n",
        "      'n_estimators': int(random.randint(*param_limits['n_estimators'])),\n",
        "      'max_depth': int(random.randint(*param_limits['max_depth'])),\n",
        "      'min_samples_split': int(random.randint(*param_limits['min_samples_split'])),\n",
        "      'min_samples_leaf': int(random.randint(*param_limits['min_samples_leaf'])),\n",
        "      'min_weight_fraction_leaf': random.uniform(*param_limits['min_weight_fraction_leaf']),\n",
        "      'max_features': int(random.randint(*param_limits['max_features'])),\n",
        "      'max_leaf_nodes': int(random.randint(*param_limits['max_leaf_nodes'])),\n",
        "      'min_impurity_decrease': random.uniform(*param_limits['min_impurity_decrease']),\n",
        "      'bootstrap': bool(random.choice((True, False))),\n",
        "      'oob_score': False,\n",
        "      'ccp_alpha': random.uniform(*param_limits['ccp_alpha']),\n",
        "      'max_samples': int(random.randint(*param_limits['max_samples'])),\n",
        "  }\n",
        "\n",
        "  if HP['bootstrap'] == True:\n",
        "    HP['oob_score'] = bool(random.choice((True, False)))\n",
        "  \n",
        "  return HP\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_train, X_cv, y_train, y_cv):\n",
        "  model.fit(X_train, y_train)\n",
        "  pred = model.predict(X_cv)\n",
        "  return mean_absolute_error(pred, y_cv)\n",
        "\n",
        "\n",
        "def ragnge_size(limits):\n",
        "  return limits[1] - limits[0]\n",
        "\n",
        "\n",
        "def small_step_mutation(zygote):\n",
        "  zygote['n_estimators'] = max(1, zygote['n_estimators'] + int(np.random.normal(0, ragnge_size(param_limits['n_estimators']) * small_step_mutation_rate)))\n",
        "  zygote['max_depth'] = min(max(1, zygote['max_depth'] + int(np.random.normal(0, int(ragnge_size(param_limits['max_depth']) * small_step_mutation_rate)))), n_features)\n",
        "  zygote['min_samples_split'] = max(2, zygote['min_samples_split'] + int(np.random.normal(0, ragnge_size(param_limits['min_samples_split']) * small_step_mutation_rate)))\n",
        "  zygote['min_samples_leaf'] = max(1, zygote['min_samples_leaf'] + int(np.random.normal(0, ragnge_size(param_limits['min_samples_leaf']) * small_step_mutation_rate)))\n",
        "  zygote['min_weight_fraction_leaf'] = min(max(0, zygote['min_weight_fraction_leaf'] + np.random.normal(0, ragnge_size(param_limits['min_weight_fraction_leaf']) * small_step_mutation_rate)), 0.5)\n",
        "  zygote['max_features'] = min(max(1, zygote['max_features'] + int(np.random.normal(0, int(ragnge_size(param_limits['max_features']) * small_step_mutation_rate)))), n_features)\n",
        "  zygote['max_leaf_nodes'] = max(2, zygote['max_leaf_nodes'] + int(np.random.normal(0, ragnge_size(param_limits['max_leaf_nodes']) * small_step_mutation_rate)))\n",
        "  zygote['min_impurity_decrease'] += max(0, np.random.normal(0, ragnge_size(param_limits['min_impurity_decrease']) * small_step_mutation_rate))\n",
        "  zygote['bootstrap'] = bool(np.random.choice([True, False], p=[1 - small_step_mutation_rate if x==zygote['bootstrap'] else small_step_mutation_rate for x in [True, False]]))\n",
        "  zygote['oob_score'] = bool(np.random.choice([True, False], p=[1 - small_step_mutation_rate if x==zygote['oob_score'] else small_step_mutation_rate for x in [True, False]]))\n",
        "  zygote['ccp_alpha'] = max(0, zygote['ccp_alpha'] + int(np.random.normal(0, ragnge_size(param_limits['ccp_alpha']) * small_step_mutation_rate)))\n",
        "  zygote['max_samples'] = min(max(1, zygote['max_samples'] + int(np.random.normal(0, ragnge_size(param_limits['max_samples']) * small_step_mutation_rate))), n_samples)\n",
        "\n",
        "  if zygote['bootstrap'] == False:\n",
        "    zygote['oob_score'] = False\n",
        "  \n",
        "  return zygote\n",
        "\n",
        "\n",
        "def should_do_large_step():\n",
        "  return np.random.choice([True, False], p=[1 - large_step_mutation_rate, large_step_mutation_rate])\n",
        "\n",
        "\n",
        "def large_step_mutation(zygote):\n",
        "  global param_limits\n",
        "\n",
        "  if should_do_large_step():\n",
        "    zygote['n_estimators'] = int(random.randint(*param_limits['n_estimators']))\n",
        "  if should_do_large_step():\n",
        "    zygote['max_depth'] = int(random.randint(*param_limits['max_depth']))\n",
        "  if should_do_large_step():\n",
        "    zygote['min_samples_split'] = int(random.randint(*param_limits['min_samples_split']))\n",
        "  if should_do_large_step():\n",
        "    zygote['min_samples_leaf'] = int(random.randint(*param_limits['min_samples_leaf']))\n",
        "  if should_do_large_step():\n",
        "    zygote['min_weight_fraction_leaf'] = random.uniform(*param_limits['min_weight_fraction_leaf'])\n",
        "  if should_do_large_step():\n",
        "    zygote['max_features'] = int(random.randint(*param_limits['max_features']))\n",
        "  if should_do_large_step():\n",
        "    zygote['max_leaf_nodes'] = int(random.randint(*param_limits['max_leaf_nodes']))\n",
        "  if should_do_large_step():\n",
        "    zygote['min_impurity_decrease'] = random.uniform(*param_limits['min_impurity_decrease'])\n",
        "  if should_do_large_step():\n",
        "    zygote['bootstrap'] = bool(random.choice((True, False)))\n",
        "  if should_do_large_step():\n",
        "    zygote['oob_score'] = bool(random.choice((True, False)))\n",
        "  if should_do_large_step():\n",
        "    zygote['ccp_alpha'] = random.uniform(*param_limits['ccp_alpha'])\n",
        "  if should_do_large_step():\n",
        "    zygote['max_samples'] = int(random.randint(*param_limits['max_samples']))\n",
        "\n",
        "  if zygote['bootstrap'] == False:\n",
        "    zygote['oob_score'] = False\n",
        "  \n",
        "  return zygote\n",
        "\n",
        "\n",
        "def mutate(zygote):\n",
        "  zygote = small_step_mutation(zygote)\n",
        "  zygote = large_step_mutation(zygote)\n",
        "  return zygote\n",
        "\n",
        "\n",
        "def fertilize(male, female):\n",
        "  parents = [male, female]\n",
        "\n",
        "  return mutate({\n",
        "      'n_estimators': random.choice(parents)['hp']['n_estimators'],\n",
        "      'max_depth': random.choice(parents)['hp']['max_depth'],\n",
        "      'min_samples_split': random.choice(parents)['hp']['min_samples_split'],\n",
        "      'min_samples_leaf': random.choice(parents)['hp']['min_samples_leaf'],\n",
        "      'min_weight_fraction_leaf': random.choice(parents)['hp']['min_weight_fraction_leaf'],\n",
        "      'max_features': random.choice(parents)['hp']['max_features'],\n",
        "      'max_leaf_nodes': random.choice(parents)['hp']['max_leaf_nodes'],\n",
        "      'min_impurity_decrease': random.choice(parents)['hp']['min_impurity_decrease'],\n",
        "      'bootstrap': random.choice(parents)['hp']['bootstrap'],\n",
        "      'oob_score': random.choice(parents)['hp']['oob_score'],\n",
        "      'ccp_alpha': random.choice(parents)['hp']['ccp_alpha'],\n",
        "      'max_samples': random.choice(parents)['hp']['max_samples'],\n",
        "  })\n",
        "\n",
        "\n",
        "def breed(population):\n",
        "  # normal = [abs(x) for x in random_gen.normal(0, n_survivors, size=len(population))]\n",
        "  # probs = sorted([x / sum(normal) for x in normal], reverse=True)\n",
        "  # male, female = random_gen.choice(population, replace=False, size=2, p=probs)\n",
        "  male, female = random_gen.choice(population[:n_survivors], replace=False, size=2)\n",
        "  return {'hp': fertilize(male, female), 'mae': None}\n",
        "\n",
        "\n",
        "def perform_drift(teritories, idx):\n",
        "  teritory_1 = teritories[idx]\n",
        "  teritory_2 = random.choice([x for x in teritories if x is not teritory_1])\n",
        "  specimen_1 = random_gen.choice(teritory_1)\n",
        "  specimen_2 = random_gen.choice(teritory_2)\n",
        "  teritory_1.remove(specimen_1)\n",
        "  teritory_2.remove(specimen_2)\n",
        "  teritory_1.append(specimen_2)\n",
        "  teritory_2.append(specimen_1)\n",
        "\n",
        "\n",
        "class Survivor:\n",
        "  def __init__(self, hp, mae):\n",
        "    self.hp = hp\n",
        "    self.mae = mae\n",
        "  \n",
        "  def __lt__(self, other):\n",
        "    return self.mae < other.mae\n",
        "  \n",
        "  def to_dict(self):\n",
        "    return {'hp': self.hp, 'mae': self.mae}\n",
        "\n",
        "\n",
        "def seconds2timestr(seconds): \n",
        "  hour = seconds // 3600\n",
        "  seconds %= 3600\n",
        "  minutes = seconds // 60\n",
        "  seconds %= 60\n",
        "  \n",
        "  return \"%d:%02d:%02d\" % (hour, minutes, seconds)\n",
        "\n",
        "\n",
        "def process_specimen(population, idx):\n",
        "  global preped_X_train, preped_X_cv, y_train, y_cv\n",
        "  if population[idx]['mae'] is None:\n",
        "    model = model_from_hp(population[idx]['hp'])\n",
        "    population[idx]['mae'] = evaluate_model(model, preped_X_train, preped_X_cv, y_train, y_cv)\n",
        "  return population[idx]\n",
        "\n",
        "\n",
        "def process_teritory(ecosystem, need_drift, idx, executor):\n",
        "    teritories = ecosystem['teritories']\n",
        "    futures = [executor.submit(process_specimen, teritories[idx], i) for i in range(len(teritories[idx]))]\n",
        "    survivors = []\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        heapq.heappush(survivors, Survivor(**future.result()))\n",
        "        print('.', end='')\n",
        "    normal = random_gen.normal(0, n_survivors, size=len(teritories[idx]))\n",
        "    abs_normal = np.abs(normal)\n",
        "    probs = abs_normal / np.sum(abs_normal)\n",
        "    sorted_probs = sorted(probs, reverse=True)\n",
        "    for i in range(n_survivors, len(sorted_probs)):\n",
        "      for j in range(n_survivors):\n",
        "        sorted_probs[j] += sorted_probs[i] / 2\n",
        "        sorted_probs[i] /= 2\n",
        "    survivors = random_gen.choice(survivors, replace=False, size=n_survivors, p=sorted_probs)\n",
        "    survivors = sorted(survivors.tolist())\n",
        "    # survivors = heapq.nsmallest(n_survivors, survivors)\n",
        "    new_avg_mae_survivors = np.mean([x.mae for x in survivors])\n",
        "    try:\n",
        "        improvement = ecosystem['avg_mae_survivors'][-2]['values'][idx] / new_avg_mae_survivors - 1\n",
        "        if improvement < drift_threshold:\n",
        "            need_drift[idx] = True\n",
        "    except:\n",
        "        improvement = 0\n",
        "    ecosystem['avg_mae_survivors'][-1]['values'][idx] = new_avg_mae_survivors\n",
        "    print(\"\\nBest MAE for teritory\", idx, \"is\", survivors[0].mae, \". Average improvement is\", improvement)\n",
        "    teritories[idx] = [x.to_dict() for x in survivors]\n",
        "    offsprings = []\n",
        "    futures = [executor.submit(breed, teritories[idx]) for i in range((pop_size - len(teritories[idx])))]\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        offsprings.append(future.result())\n",
        "    teritories[idx] += offsprings\n",
        "    print(\"Finished evaluating teritory\", idx)\n",
        "    return improvement\n",
        "\n",
        "\n",
        "try:\n",
        "  with open(os.path.join(project_files_path, 'ecosystem.json')) as f:\n",
        "    ecosystem = json.load(f)\n",
        "    generations_passed = ecosystem['last_generation'] + 1\n",
        "except Exception as e:\n",
        "  for i in range(n_teritories):\n",
        "    ecosystem['teritories'].append([{'hp': init_HP(), 'mae': None} for i in range(int(pop_size))])\n",
        "  generations_passed = 0\n",
        "\n",
        "# Supress warnings to avoid terminal clutter\n",
        "logging.captureWarnings(True)\n",
        "\n",
        "\n",
        "# def rec_eco(ecosystem):\n",
        "#   for key in ecosystem:\n",
        "#     if type(ecosystem[key]) == np.ndarray:\n",
        "#       print(key, \"is an ndarray\")\n",
        "#     else:\n",
        "#       try:\n",
        "#         print('iterating', key)\n",
        "#         rec_eco(ecosystem[key])\n",
        "#       except:\n",
        "#         continue\n",
        "\n",
        "\n",
        "# times = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  for generation in range(generations_passed, n_generations):\n",
        "    print(\"*\" * 100, \"\\nGeneration\", str(generation) + '/' + str(n_generations))\n",
        "    start_time = time.time()\n",
        "    need_drift = [False] * n_teritories\n",
        "    ecosystem['avg_mae_survivors'].append({'generation': generation, 'values':[0] * n_teritories})\n",
        "    improvements = []\n",
        "    futures = [executor.submit(process_teritory, ecosystem, need_drift, idx, executor) for idx in range(len(ecosystem['teritories']))]\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "      improvements.append(future.result())\n",
        "    average_improve = float(np.mean(improvements))\n",
        "    ecosystem['average_total_improve'].append(ecosystem['average_total_improve'][-1] + average_improve)\n",
        "    ecosystem['improvements'].append(average_improve)\n",
        "    sum_drifts = sum(need_drift)\n",
        "    # n_survivors = max(MIN_SURVIVORS, min(pop_size - 5, n_survivors - 2 + sum_drifts))\n",
        "    small_step_mutation_rate = 0.1 * (sum_drifts + 1)\n",
        "    large_step_mutation_rate = 0.01 * (sum_drifts + 1)\n",
        "    print(\"Need drift:\", sum_drifts)\n",
        "    for i in range(len(need_drift)):\n",
        "      if need_drift[i] is True:\n",
        "        perform_drift(ecosystem['teritories'], i)\n",
        "    ecosystem['last_generation'] = generation\n",
        "    end_time = time.time()\n",
        "    generation_process_time = end_time - start_time\n",
        "    ecosystem['runtime_running_avg'] = (ecosystem['runtime_running_avg'] + generation_process_time) / 2\n",
        "    ecosystem['times'].append(generation_process_time)\n",
        "    # rec_eco(ecosystem)\n",
        "    with open(os.path.join(project_files_path, 'ecosystem.json'), 'w') as f:\n",
        "      json.dump(ecosystem, f)\n",
        "    remaining_generations = n_generations - generation - 1\n",
        "    print(\"This generation took\", seconds2timestr(generation_process_time), \"to process.\")\n",
        "    print(\"Remaining\", remaining_generations, \"generations and approximately\", seconds2timestr(ecosystem['runtime_running_avg'] * remaining_generations))\n",
        "    print(\"Average improvement for generation:\", average_improve, \". Average total improvement:\", ecosystem['average_total_improve'][-1])\n",
        "    plt.plot(ecosystem['improvements'])\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Avg. improvement per gen.\")\n",
        "    plt.show()\n",
        "    plt.plot(ecosystem['average_total_improve'])\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Total improvement\")\n",
        "    plt.show()\n",
        "    # print(\"n_survivors:\", n_survivors)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNpu1HU4zQpm"
      },
      "source": [
        "# Prepeare test predictions\n",
        "with open(os.path.join(project_files_path, 'ecosystem.json')) as f:\n",
        "    ecosystem = json.load(f)\n",
        "\n",
        "X_train_cv = pd.concat([preped_X_train, preped_X_cv], axis=0)\n",
        "y_train_cv = pd.concat([y_train, y_cv], axis=0)\n",
        "\n",
        "best_hp = ecosystem['teritories'][0][0]['hp']\n",
        "best_mae = ecosystem['teritories'][0][0]['mae']\n",
        "for teritory in ecosystem['teritories']:\n",
        "  for specimen in teritory:\n",
        "    if specimen['mae'] is not None and specimen['mae'] < best_mae:\n",
        "      best_mae = specimen['mae']\n",
        "      best_hp = specimen['hp']\n",
        "\n",
        "print(best_hp)\n",
        "\n",
        "model = model_from_hp(best_hp)\n",
        "\n",
        "print(\"Fitting\")\n",
        "\n",
        "model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "print(\"Predicting\")\n",
        "\n",
        "preds_test = model.predict(preped_X_test)\n",
        "\n",
        "print(\"Saving\")\n",
        "\n",
        "output = pd.DataFrame({'Id': preped_X_test.Id.astype('int32'),\n",
        "                       'SalePrice': preds_test})\n",
        "output.to_csv(os.path.join(project_files_path, 'submission_gen.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGzeA83Xo6UJ"
      },
      "source": [
        "output = pd.DataFrame({'Id': preped_X_test.Id.astype('int32'),\n",
        "                       'SalePrice': preds_test})\n",
        "output.to_csv(os.path.join(project_files_path, 'submission_gen.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}